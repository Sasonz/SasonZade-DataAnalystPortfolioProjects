{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPI0G2BM/OQfLrkNQewFVxq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OkVzKkYD-Wcx","executionInfo":{"status":"ok","timestamp":1743585172833,"user_tz":-180,"elapsed":2075,"user":{"displayName":"Sason Zade","userId":"15696081868748133077"}},"outputId":"9811a714-bf17-4dc2-de33-80aeddc383b2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","All Data:\n","         company                  location        industry  total_laid_off  \\\n","0      F-Secure   ['Helsinki', 'Non-U.S.']        Security            70.0   \n","1          #Paid   ['Toronto', 'Non-U.S.']       Marketing            19.0   \n","2      1K Kirana  ['Gurugram', 'Non-U.S.']          Retail           600.0   \n","3        23andMe           ['SF Bay Area']      Healthcare            71.0   \n","4        23andMe           ['SF Bay Area']      Healthcare            75.0   \n","...          ...                       ...             ...             ...   \n","1734      iRobot                ['Boston']        Consumer           350.0   \n","1735   iSpecimen                ['Boston']      Healthcare             NaN   \n","1736     inDrive           ['SF Bay Area']  Transportation             NaN   \n","1737     mPharma     ['Accra', 'Non-U.S.']      Healthcare           150.0   \n","1738       nCino            ['Wilmington']         Finance           100.0   \n","\n","      percentage_laid_off        date     stage        country  funds_raised  \n","0                     NaN  2023-10-25  Post-IPO        Finland           NaN  \n","1                    0.17  2023-01-27  Series B         Canada          21.0  \n","2                    0.40  2023-04-04  Series B          India           NaN  \n","3                    0.11  2023-08-08  Post-IPO  United States        1100.0  \n","4                    0.09  2023-06-09  Post-IPO  United States        1100.0  \n","...                   ...         ...       ...            ...           ...  \n","1734                 0.31  2024-01-29  Post-IPO  United States          30.0  \n","1735                 0.20  2023-09-06  Post-IPO  United States          31.0  \n","1736                 0.10  2023-08-01   Unknown  United States         387.0  \n","1737                  NaN  2023-09-04  Series D          Ghana          90.0  \n","1738                 0.07  2023-01-18  Post-IPO  United States        1100.0  \n","\n","[1739 rows x 9 columns]\n","\n","Maximum Layoffs:\n","   MAX(total_laid_off)\n","0              15000.0\n","Analysis saved to Excel successfully.\n"]}],"source":["from google.colab import drive\n","import pandas as pd\n","import sqlite3\n","from pandasql import sqldf\n","\n","# Step 1: Mount Google Drive to access files stored in the user's Google Drive\n","# This allows the script to read and write files stored in Google Drive, making it accessible in Google Colab.\n","drive.mount('/content/drive')\n","\n","# Step 2: Load the File into a Pandas DataFrame\n","# Define the file path to the Excel file stored in Google Drive.\n","# Ensure that the file exists and that the correct path is provided.\n","file_path = '/content/drive/My Drive/Data_Analyst/Portfolio_Projects/Projects/SQL/Layoffs_Dataset_After_Cleaning.xlsx'\n","\n","# Read the Excel file into a Pandas DataFrame.\n","# The 'openpyxl' engine is specified to handle Excel (.xlsx) files.\n","df = pd.read_excel(file_path, engine='openpyxl')\n","\n","# Step 3: Execute SQL Queries Using pandasql\n","# `pandasql` allows SQL queries to be executed on Pandas DataFrames.\n","query1 = \"SELECT * FROM df;\"  # Retrieve all data from the DataFrame.\n","query2 = \"SELECT MAX(total_laid_off) FROM df;\"  # Find the maximum number of layoffs in the dataset.\n","\n","# Execute the queries using pandasql and store the results in variables.\n","result1 = sqldf(query1, globals())\n","result2 = sqldf(query2, globals())\n","\n","# Display the results.\n","print(\"All Data:\")\n","print(result1)\n","print(\"\\nMaximum Layoffs:\")\n","print(result2)\n","\n","# Step 4: Use SQLite for More Efficiency\n","# SQLite is used to improve query performance and manage large datasets efficiently.\n","conn = sqlite3.connect(\":memory:\")  # Create an in-memory SQLite database.\n","\n","# Store the DataFrame as a SQL table named 'layoffs_staging2'.\n","df.to_sql(\"layoffs_staging2\", conn, index=False, if_exists=\"replace\")\n","\n","# Define multiple SQL queries for data analysis.\n","query1_sqlite = \"SELECT * FROM layoffs_staging2;\"  # Retrieve all records.\n","query2_sqlite = \"SELECT MAX(total_laid_off) FROM layoffs_staging2;\"  # Find max layoffs.\n","query3_sqlite = \"SELECT MAX(percentage_laid_off), MIN(percentage_laid_off) FROM layoffs_staging2 WHERE percentage_laid_off IS NOT NULL;\"  # Find max & min percentage layoffs.\n","query4_sqlite = \"SELECT * FROM layoffs_staging2 WHERE percentage_laid_off = 1;\"  # Get companies with 100% layoffs.\n","query5_sqlite = \"SELECT * FROM layoffs_staging2 WHERE percentage_laid_off = 1 ORDER BY funds_raised DESC;\"  # 100% layoffs ordered by funds raised.\n","query6_sqlite = \"SELECT company, total_laid_off FROM layoffs_staging2 ORDER BY total_laid_off DESC LIMIT 5;\"  # Top 5 companies by layoffs.\n","query7_sqlite = \"SELECT company, SUM(total_laid_off) FROM layoffs_staging2 GROUP BY company ORDER BY SUM(total_laid_off) DESC LIMIT 10;\"  # Top 10 companies by total layoffs.\n","query8_sqlite = \"SELECT location, SUM(total_laid_off) FROM layoffs_staging2 GROUP BY location ORDER BY SUM(total_laid_off) DESC LIMIT 10;\"  # Top 10 locations by layoffs.\n","query9_sqlite = \"SELECT country, SUM(total_laid_off) FROM layoffs_staging2 GROUP BY country ORDER BY SUM(total_laid_off) DESC;\"  # Layoffs by country.\n","query10_sqlite = \"SELECT strftime('%Y', date) AS year, SUM(total_laid_off) FROM layoffs_staging2 GROUP BY year ORDER BY year ASC;\"  # Layoffs by year.\n","query11_sqlite = \"SELECT industry, SUM(total_laid_off) FROM layoffs_staging2 GROUP BY industry ORDER BY SUM(total_laid_off) DESC;\"  # Layoffs by industry.\n","query12_sqlite = \"SELECT stage, SUM(total_laid_off) FROM layoffs_staging2 GROUP BY stage ORDER BY SUM(total_laid_off) DESC;\"  # Layoffs by stage.\n","query13_sqlite = \"\"\"\n","WITH Company_Year AS (\n","  SELECT company, strftime('%Y', date) AS years, SUM(total_laid_off) AS total_laid_off\n","  FROM layoffs_staging2\n","  GROUP BY company, years\n","),\n","Company_Year_Rank AS (\n","  SELECT company, years, total_laid_off, DENSE_RANK() OVER (PARTITION BY years ORDER BY total_laid_off DESC) AS ranking\n","  FROM Company_Year\n",")\n","SELECT company, years, total_laid_off, ranking\n","FROM Company_Year_Rank\n","WHERE ranking <= 3 AND years IS NOT NULL\n","ORDER BY years ASC, total_laid_off DESC;\n","\"\"\"  # Top 3 layoffs per year.\n","query14_sqlite = \"\"\"\n","SELECT SUBSTRING(date,1,7) as dates, SUM(total_laid_off) AS total_laid_off\n","FROM layoffs_staging2\n","GROUP BY dates\n","ORDER BY dates ASC;\n","\"\"\"  # Monthly layoffs.\n","query15_sqlite = \"\"\"\n","WITH DATE_CTE AS (\n","  SELECT SUBSTRING(date,1,7) as dates, SUM(total_laid_off) AS total_laid_off\n","  FROM layoffs_staging2\n","  GROUP BY dates\n","  ORDER BY dates ASC\n",")\n","SELECT dates, SUM(total_laid_off) OVER (ORDER BY dates ASC) as rolling_total_layoffs\n","FROM DATE_CTE\n","ORDER BY dates ASC;\n","\"\"\"  # Rolling layoffs trend.\n","\n","# Execute queries and store results.\n","result1_sqlite = pd.read_sql(query1_sqlite, conn)\n","result2_sqlite = pd.read_sql(query2_sqlite, conn)\n","result3_sqlite = pd.read_sql(query3_sqlite, conn)\n","result4_sqlite = pd.read_sql(query4_sqlite, conn)\n","result5_sqlite = pd.read_sql(query5_sqlite, conn)\n","result6_sqlite = pd.read_sql(query6_sqlite, conn)\n","result7_sqlite = pd.read_sql(query7_sqlite, conn)\n","result8_sqlite = pd.read_sql(query8_sqlite, conn)\n","result9_sqlite = pd.read_sql(query9_sqlite, conn)\n","result10_sqlite = pd.read_sql(query10_sqlite, conn)\n","result11_sqlite = pd.read_sql(query11_sqlite, conn)\n","result12_sqlite = pd.read_sql(query12_sqlite, conn)\n","result13_sqlite = pd.read_sql(query13_sqlite, conn)\n","result14_sqlite = pd.read_sql(query14_sqlite, conn)\n","result15_sqlite = pd.read_sql(query15_sqlite, conn)\n","\n","# Save results to an Excel file with multiple sheets.\n","output_path = '/content/drive/My Drive/Data_Analyst/Portfolio_Projects/Projects/SQL/Layoffs_Dataset_After_Analysis.xlsx'\n","with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n","    result1_sqlite.to_excel(writer, sheet_name='All Data', index=False)\n","    result2_sqlite.to_excel(writer, sheet_name='Max Layoffs', index=False)\n","    result3_sqlite.to_excel(writer, sheet_name='Max Min % Laid Off', index=False)\n","    result4_sqlite.to_excel(writer, sheet_name='100% Layoffs', index=False)\n","    result5_sqlite.to_excel(writer, sheet_name='100% Layoffs by Funds', index=False)\n","    result6_sqlite.to_excel(writer, sheet_name='Top 5 Single-Day', index=False)\n","    result7_sqlite.to_excel(writer, sheet_name='Top 10 Companies', index=False)\n","    result8_sqlite.to_excel(writer, sheet_name='Top 10 Locations', index=False)\n","    result9_sqlite.to_excel(writer, sheet_name='Layoffs by Country', index=False)\n","    result10_sqlite.to_excel(writer, sheet_name='Layoffs by Year', index=False)\n","    result11_sqlite.to_excel(writer, sheet_name='Layoffs by Industry', index=False)\n","    result12_sqlite.to_excel(writer, sheet_name='Layoffs by Stage', index=False)\n","    result13_sqlite.to_excel(writer, sheet_name='Top 3 by Year', index=False)\n","    result14_sqlite.to_excel(writer, sheet_name='Monthly Layoffs', index=False)\n","    result15_sqlite.to_excel(writer, sheet_name='Rolling Layoffs', index=False)\n","\n","print(\"Analysis saved to Excel successfully.\")\n"]}]}